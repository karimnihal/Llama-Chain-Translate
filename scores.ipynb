{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nk13/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import sacrebleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.chrf_score import corpus_chrf\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare hypothesis and reference sentences from files\n",
    "with open('hypothesis.txt', 'r') as hyp_file:\n",
    "    hypotheses = [line.strip() for line in hyp_file]\n",
    "\n",
    "with open('improved_translation.txt', 'r') as imp_file:\n",
    "    improved = [line.strip() for line in imp_file]\n",
    "\n",
    "with open('reference.txt', 'r') as ref_file:\n",
    "    references = [[line.strip()] for line in ref_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 3.4586\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(hypotheses, references)\n",
    "print(f\"BLEU Score: {bleu.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# reference = \"The quick brown fox jumps over the lazy dog.\".split()\n",
    "# candidate = \"A fast brown fox leaps over a lazy dog.\".split()\n",
    "\n",
    "# score = meteor_score([reference], candidate)\n",
    "# print(f\"METEOR Score: {score:.4f}\")\n",
    "\n",
    "# METEOR Score - Compute for each sentence pair and get the average\n",
    "score = meteor_score(references, hypotheses)\n",
    "print(f\"METEOR Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHRF Score: 0.2170\n"
     ]
    }
   ],
   "source": [
    "# references = [[\"this is a reference sentence\"], [\"another reference sentence\"]]\n",
    "# hypotheses = [\"this is a hypothesis sentence\", \"another hypothesis sentence\"]\n",
    "\n",
    "# chrf_score = corpus_chrf(references, hypotheses)\n",
    "# print(f\"CHRF Score: {chrf_score}\")\n",
    "\n",
    "# CHRF Score\n",
    "chrf_score = corpus_chrf(references, hypotheses)\n",
    "print(f\"CHRF Score: {chrf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"evaluations\": {\n",
    "        \"prompt\": 'Consider yourself to be a human evaluator who is well versed in the {source_lang} language and the {target_lang} language.\\nEvaluate the given translation for adequacy and fluency and explain why those scores were given\\n{source_lang} segment: {source_seg}\\n{target_lang} translation: {target_seg}',\n",
    "        \"validate_answer\": lambda x: x\n",
    "    },\n",
    "    \"translations\": {\n",
    "        \"prompt\": 'Consider yourself to be a human translator who is well versed in the {source_lang} language and the {target_lang} language. An expert evaluator has previously given the following evaluation on this translation -\\n{source_lang} segment: {source_seg}\\n{target_lang} translation: {target_seg}\\nEvaluation: {evaluation}\\nBased on this evaluation give an improved translation. Output nothing but the improved translation.\\nImproved Translation: ',\n",
    "        \"validate_answer\": lambda x: parse_translation(x)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluations Prompt:\n",
      "Consider yourself to be a human evaluator who is well versed in the {source_lang} language and the {target_lang} language.\n",
      "Evaluate the given translation for adequacy and fluency and explain why those scores were given\n",
      "{source_lang} segment: {source_seg}\n",
      "{target_lang} translation: {target_seg}\n",
      "\n",
      "Translations Prompt:\n",
      "Consider yourself to be a human translator who is well versed in the {source_lang} language and the {target_lang} language. An expert evaluator has previously given the following evaluation on this translation -\n",
      "{source_lang} segment: {source_seg}\n",
      "{target_lang} translation: {target_seg}\n",
      "Evaluation: {evaluation}\n",
      "Based on this evaluation give an improved translation. Output nothing but the improved translation.\n",
      "Improved Translation: \n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluations Prompt:\")\n",
    "print(prompts['evaluations']['prompt'])\n",
    "\n",
    "print(\"\\nTranslations Prompt:\")\n",
    "print(prompts['translations']['prompt'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
